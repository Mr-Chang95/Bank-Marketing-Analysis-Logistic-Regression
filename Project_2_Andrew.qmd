---
title: "Project_2"
author: "Andrew Yule"
format: 
  html: 
    self-contained: true
editor: visual
editor_options: 
  chunk_output_type: console
---

## Library Loading

```{r}
#| output: false
library(GGally)
library(caret)
library(pROC)
library(gtsummary)
library(tidyverse)

theme_set(theme_bw())
```

## Data import, variable renaming, and data cleaning

```{r}
#| output: false
# Load in the data
bank = read_csv("/Users/andrewyule/Dropbox/SMU MSDS/06 - Applied Statistics/Project 2/bank-full.csv")

# Rename variables for better clarity on what they represent
#colnames(bank) = c("age", "job_category", "marital_status", "education_level", "previous_default", "account_balance", "home_owner", "current_loan", "contact_method", "day_of_month", "month", "call_duration", "campaign_number", "days_from_previous_contact", "number_of_previous_contacts", "previous_outcome", "subscribed")

# Any data cleaning steps can be added here as they are determined


```

## Introduction to the Data

This section focuses on individual variables to help famliarize oneself with the data. Exploratory data analysis to look at potential relationships in the data is in the next section.

#### How much missing data is there?

There is no pure missing data found in the data. However, there are expected to be categorical variables marked as "unknown".
```{r}
sum(is.na(bank))
```

#### How many total people were successfully marketed to?

The majority of contacts were not successful (88%) with only 12% being successful.
```{r}
bank |>
  group_by(y) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### Summary of the data

```{r}
summary(bank)
```

#### What is the distribution of Ages?

The distribution of age is slightly right skewed, but reasonably close to normal.
```{r}
bank |>
  ggplot(aes(x = age)) +
  geom_histogram() +
  labs(title = "Distribution of Age", x = "Age", y = "Count")
```

#### How many job categories do we have and what are they?

There are 12 job categories within the data set, each being listed below. Of the 12 categories, one captures "unknown" job categories which represents less than 1% of the total data. The highest number of job categories are "blue-collar", "management", and "technician".
```{r}
bank |>
  group_by(job) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the proportion of people married vs. single?

Just over 60% of the people were married, while 28% were single, and the remaining 12% were divorced. No missing martial status values were found.
```{r}
bank |>
  group_by(marital) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the breakdown of education levels?

Over half of the people in the dataset (51%) had secondary education levels. Almost 30% had tertiary education levels. Finally, 15% of people had only primary (high-school) education levels.
```{r}
bank |>
  group_by(education) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the percentage of people that have defaulted?

Less than 2% of people in the dataset had ever defaulted and the remaining 98% had not.
```{r}
bank |>
  group_by(default) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the distribution of account balance?

The distribution of account balance is very right skewed. Applying a log transformation to the data helps to produce an approximately normal distribution.
```{r}
bank |>
  ggplot(aes(x = balance)) +
  geom_histogram() +
  labs(title = "Distribution of Account Balance", x = "Balance", y = "Count")

# Log transformation
bank |>
  ggplot(aes(x = balance)) +
  geom_histogram() +
  labs(title = "Distribution of Account Balance", x = "Balance", y = "Count") +
  scale_x_log10()
```

#### What is the percentage of people that have housing?

The breakdown is roughly split with 55% of people having housing and the remaining 44% not having housing currently.
```{r}
bank |>
  group_by(housing) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the percentage of people that have loans?

The majority of people (84%) do not have a current loan.
```{r}
bank |>
  group_by(loan) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What is the breakdown of how people were contacted?

The majority of people were contacted by way of their mobile phones (65%) as compared to home phones (28.8). For over 28% of people, the information was not available.
```{r}
bank |>
  group_by(contact) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

#### What day of the month were individuals contacted the most?

With the exception of day 20 in a month which had a higher number of contacts, most other days look to follow a uniform distribution with all days having equal representation.
```{r}
bank |>
  count(day) |>
  ggplot(aes(x = day, y = n)) +
  geom_col() +
  labs(title = "Number of Times Individuals were Contacted on Various Days of the Month", x = "Day", y = "Total Number of Contacts")
```

#### What month were individuals contacted the most?

May was by far saw the highest number of contacts. Many months like December, March, October, and September, saw relatively small amounts of contacts.
```{r}
bank |>
  count(month) |>
  ggplot(aes(x = month, y = n)) +
  geom_col() +
  labs(title = "Number of Times Individuals were Contacted in Various Months", x = "Month", y = "Total Number of Contacts")
```

#### What is the distribution of duration?

The distribution of duration is very right skewed. Applying a log transformation to the data helps to produce an approximately normal distribution.
```{r}
bank |>
  ggplot(aes(x = duration)) +
  geom_histogram() +
  labs(title = "Distribution of Phone Call Duration", x = "Duration", y = "Count")

# Log transformation
bank |>
  ggplot(aes(x = duration)) +
  geom_histogram() +
  labs(title = "Distribution of Phone Call Duration", x = "Duration", y = "Count") +
  scale_x_log10()
```

#### How many total campaigns were there and what is the breakdown?

In total there were 63 campaigns, however, the vast majority of people were contacted in the first few campaigns and it trails off very quickly.
```{r}
max(bank$campaign)

bank |>
  count(campaign) |>
  ggplot(aes(x = campaign, y = n)) +
  geom_col() +
  labs(title = "Number of Times Individuals were Contacted in Various Months", x = "Month", y = "Total Number of Contacts")
```

#### What is the distribution of days from previous contact?

The distribution of previous days is very right skewed.
```{r}
bank |>
  ggplot(aes(x = pdays)) +
  geom_histogram() +
  labs(title = "Distribution of Previous Days", x = "PDays", y = "Count")

# Log transformation
bank |>
  ggplot(aes(x = pdays)) +
  geom_histogram() +
  labs(title = "Distribution of Previous Days", x = "PDays", y = "Count") +
  scale_x_log10()
```

#### What is the distribution of previous campaign?

The distribution of previous campaign is very right skewed, which aligns with the campaigns.
```{r}
bank |>
  ggplot(aes(x = previous)) +
  geom_histogram() +
  labs(title = "Distribution of Previous Campaign", x = "Previous Campaign", y = "Count")

# Log transformation
bank |>
  ggplot(aes(x = previous)) +
  geom_histogram() +
  labs(title = "Distribution of Previous Campaign", x = "Previous Campaign", y = "Count") +
  scale_x_log10()
```

#### What is the breakdown of poutcome?

The 82% of outcomes were not known, 11% were failures, 4% were classified as "other", and finally, only 3% were found to be success.
```{r}
bank |>
  group_by(previous) |>
  summarise(n = n()) |>
  mutate(Percent = 100 * (n / sum(n)))
```

## Multiple Variable EDA

#### Running through each variable and their relationship with the subscription success

```{r}
# yes or no subscription count
ggplot(bank, aes(x = y)) +
  geom_bar() +
  labs(title = "Breakdown of Clients Subscribing to a Term Deposit", x = "Subscribed to a Term Deposit", y = "Total Number")

ggsave("Downloads/Plot1.png")

# Job Count
ggplot(bank, aes(x = job, fill = y)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Education Count
ggplot(bank, aes(x = education, fill = y)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Age Distribution
ggplot(bank, aes(x = age, fill = y)) +
  geom_histogram(bins = 50)

# Duration Distribution
ggplot(bank, aes(x = duration, fill = y)) +
  geom_histogram(bins = 50)

# Balance Distribution
ggplot(bank, aes(x = balance, fill = y)) +
  geom_histogram(bins = 20)

# Housing Count
ggplot(bank, aes(x = housing, fill = y)) +
  geom_bar()

# Loan 
ggplot(data = bank, aes(x = loan , fill = y)) + 
  geom_bar(stat = 'count', position = 'dodge') + 
  xlab("Loan")+ylab("Response")

# Marital Status 
ggplot(data = bank, aes(x = marital, fill = y)) + 
  geom_bar(stat = 'count', position = 'dodge') + 
  xlab("Marital")+ylab("Response")

# Default
ggplot(data = bank, aes(x = default,  fill = y)) + 
  geom_bar(stat = 'count', position = 'dodge') + 
  xlab("Default")+ylab("Response")

# Outcome
ggplot(data = bank, aes(x = poutcome,  fill = y)) + 
  geom_bar(stat = 'count', position = 'dodge') + 
  xlab("Poutcome")+ylab("Response")

# Previous 
ggplot(data = bank, aes(x = previous, fill = y)) +
  geom_histogram() +
  labs(title = "Distribution of Previous Campaign", 
       x = "Previous Campaign", y = "Count")
```

#### LOESS Plots

```{r}
# Make new numeric column for "y" column
bank$y_num = num(ifelse(bank$y == "yes",1,0))

# Age (check)
ggplot(bank, aes(x = age, y = y_num)) +geom_point() +
  geom_smooth(method = "loess", span =.75, se = FALSE)

ggplot(bank,aes(x=age,y=y_num, color = marital))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=age,y=y_num, color = education))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE) # no difference 

ggplot(bank,aes(x=age,y=y_num, color = housing))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=age,y=y_num, color = loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

# Balance
ggplot(bank,aes(x=balance,y=y_num))+geom_point()+
  geom_smooth(method="loess",size=1,span=0.1, se = FALSE)

ggplot(bank,aes(x=balance,y=y_num, color = marital))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=balance,y=y_num, color = housing))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE) # no difference

ggplot(bank,aes(x=balance,y=y_num, color = education))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=balance,y=y_num, color = loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

# Duration (maybe add poly to it)
ggplot(bank,aes(x=duration,y=y_num))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=duration,y=y_num, color = marital))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=duration,y=y_num, color = housing))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=duration,y=y_num, color = education))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

ggplot(bank,aes(x=duration,y=y_num, color = loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=.75, se = FALSE)

# Pdays (check)
ggplot(bank,aes(x=pdays,y=y_num))+geom_point()+
  geom_smooth(method="loess",size=1,span=1, se = FALSE)

ggplot(bank,aes(x=pdays,y=y_num, color = marital))+geom_point()+
  geom_smooth(method="loess",size=1,span=1, se = FALSE)

ggplot(bank,aes(x=pdays,y=y_num, color = housing))+geom_point()+
  geom_smooth(method="loess",size=1,span=1, se = FALSE)

ggplot(bank,aes(x=pdays,y=y_num, color = education))+geom_point()+
  geom_smooth(method="loess",size=1,span=1, se = FALSE)

ggplot(bank,aes(x=pdays,y=y_num, color = loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=1, se = FALSE)
```


## Train / test split

#### Let's make sure to split our data using a 70/30 training/test ratio. Any model builds moving forward should utilize the training data set, while key performance metrics should be performed on the test data set.

```{r}
#bank = select(bank, c(-y_num))
set.seed(124)
trainIndex = createDataPartition(bank$y, p= .7, list = F) 
training = bank[trainIndex,]
test = bank[-trainIndex,]
```

## Objective 1

#### Basic model using all of the data to start with

AUROC:          0.9085
LogLoss:        0.2409315
AIC:            15206
Accuracy:       0.814
Sensitivity:    0.8064
Specificity:    0.8714

Note that because we have such unbalanced data, our accuracy is always expected to be high. Only 10% of the results were "yes", so just picking "no" 100% of the time would result in an accuracy of 90%.

However, once we try and increase the specificity, we realize that the ideal threshold for this base model is 0.096, which reduces our accuracy down to 81%. The decrease in accuracy results in a much better improvement and equality in sensitivity and specificity.

```{r}
# Create a base model using all the variables, record LogLoss and AIC metrics
set.seed(1234)
fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 1, classProbs = TRUE, summaryFunction = mnLogLoss)
logit_base = train(factor(y) ~ ., data = training, method = "glm", trControl = fitControl, metric = "logLoss")
summary(logit_base$finalModel)
length(predictors(logit_base))

# Generate an ROC curve to help us determine the best threshold
logit_base_predictions = predict(logit_base, test, type = "prob")$yes
logit_base_roc = roc(response = test$y, predictor = logit_base_predictions, levels = c("no", "yes"))
auc(logit_base_roc)
plot(logit_base_roc, print.thres = "best", col = "black")

# Predict
threshold = 0.096
logit_base_predictions_categorical = factor(ifelse(predict(logit_base, test, type = "prob")$yes >= threshold, "yes", "no"), levels = c("no", "yes"))

confusionMatrix(data = logit_base_predictions_categorical, reference = factor(test$y, levels = c("no", "yes")))
```

#### Feature selection (stepwise regression) to help reduce the overall number of predictors used by the model

NOTE: The final model was produced elsewhere through a manual stepping process as some of the automated routines failed

```{r}
# Export the training data set for stepwise evaluation
write_csv(training, "Downloads/training.csv")

# Show stepwise results
stepwiseResults = read_csv("/Users/andrewyule/Dropbox/SMU MSDS/06 - Applied Statistics/Project 2/Stepwise Regression Results.csv")
ggplot(stepwiseResults, aes(x = reorder(`Variable Added`, AIC, decreasing = T), y = AIC)) + 
  geom_point(size = 3) +
  labs(title = "AIC Metric with Additional Variables being Added", x= "Variable Added", y = "AIC") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggsave("Downloads/Plot2.png")

# Create the stepwise model
logit_step = glm(factor(y) ~ poutcome + duration + month + contact + housing, data = training, family = "binomial")
summary(logit_step)

# Create a nice looking table
tbl_regression(logit_step, exponentiate = TRUE) |> 
  as_gt() |> 
  gt::tab_options(table.font.size = 10)

# Generate an ROC curve to help us determine the best threshold
logit_step_predictions = predict(logit_step, test, type = "response")
logit_step_roc = roc(response = test$y, predictor = logit_step_predictions, levels = c("no", "yes"))
auc(logit_step_roc)
plot(logit_step_roc, print.thres = "best", col = "black")

# Predict
threshold = 0.101
logit_step_predictions_categorical = factor(ifelse(predict(logit_step, test, "response") >= threshold, "yes", "no"), levels = c("no", "yes"))

confusionMatrix(data = logit_step_predictions_categorical, reference = factor(test$y, levels = c("no", "yes")))
```

#### Feature selection (penalized regression) to help reduce the overall number of predictors used by the model

NOTE: This produces a model that is much more complex than the stepwise while resulting in nearly the same overall performance metrics

```{r}
set.seed(1234)
#fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 1, classProbs = TRUE, summaryFunction = mnLogLoss)
fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 1)
lambda = seq(0,.05,.001)
#alpha = seq(0, 1, 0.1)
logit_glmnet = train(factor(y) ~ ., data = training, method = "glmnet", trControl = fitControl, metric = "logLoss", tuneGrid = expand.grid(.alpha = 1, .lambda = lambda))
coef(logit_glmnet$finalModel, logit_glmnet$bestTune$lambda)
coef(logit_glmnet$finalModel, logit_glmnet$bestTune$lambda)
length(predictors(logit_glmnet))
plot(logit_glmnet)

logit_glmnet$finalModel$
logit_glmnet$finalModel$lambdaOpt
```

## Objective 2

#### 1. More complicated logistic regression model

```{r}
set.seed(1234)
fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 1, classProbs = TRUE, summaryFunction = mnLogLoss)
logit_complex = train(factor(y) ~ age + job + marital + education + default + balance + housing + loan + contact + day + month + log(duration+1) + campaign + pdays + previous + poutcome, data = training, method = "glm", trControl = fitControl, metric = "logLoss")
summary(logit_complex$finalModel)
length(predictors(logit_complex))

# Generate an ROC curve to help us determine the best threshold
logit_complex_predictions = predict(logit_complex, test, type = "prob")$yes
logit_complex_roc = roc(response = test$y, predictor = logit_complex_predictions, levels = c("no", "yes"))
auc(logit_complex_roc)
plot(logit_complex_roc, print.thres = "best", col = "black")

# Predict
threshold = 0.096
logit_complex_predictions_categorical = factor(ifelse(predict(logit_complex, test, type = "prob")$yes >= threshold, "yes", "no"), levels = c("no", "yes"))

confusionMatrix(data = logit_complex_predictions_categorical, reference = factor(test$y, levels = c("no", "yes")))

```

#### 2. LDA or QDA model

```{r}

```


#### 3. Non-parametric model

```{r}

```


















